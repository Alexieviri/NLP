{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1\n",
    "---\n",
    "Требования:\n",
    "* Python >= 3.X\n",
    "* NLTK >= 3.2.5\n",
    "\n",
    "Лабораторную работу необходимо выполнять в данном шаблоне. Результат работы выслать письмом на litvinov.vg@ssau.ru. В теме письма указывать ФИО полностью.\n",
    "\n",
    "Для выполнения задания необходимо подобрать корпус текстов (художественных) на английском языке. Объем корпуса должен составлять не менее $3 \\cdot 10^7$ символов. Далее все действия будут выполнятся над выбранными данными. Для отладки алгоритмов лучше использовать не весь корпус, а лишь его часть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from collections import Counter \n",
    "import nltk\n",
    "from nltk import RegexpTokenizer, ngrams\n",
    "from nltk import word_tokenize, wordpunct_tokenize\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Использованные книги:\n",
    "---\n",
    "1. [GIVE THE BOYS A GREAT BIG HAND](https://www.rulit.me/books/give-the-boys-a-great-big-hand-download-679500.html)\n",
    "2. [Treasure Island by Robert Louis Stevenson](https://www.gutenberg.org/ebooks/120) \n",
    "3. [Great Expectations by Charles Dickens](https://www.gutenberg.org/ebooks/1400) \n",
    "4. [Hamlet, Prince of Denmark by William Shakespeare](https://www.gutenberg.org/ebooks/1524) \n",
    "5. [TUBE TO NOWHERE](https://www.rulit.me/books/tube-to-nowhere-download-679490.html)\n",
    "6. [YOU HAVE YOURSELF A DEAL](https://www.rulit.me/books/you-have-yourself-a-deal-download-679488.html)\n",
    "7. [THE WAY THE COOKIE CRUMBLES](https://www.rulit.me/books/the-way-the-cookie-crumbles-download-679487.html) \n",
    "8. [YOU MUST BE KIDDING](https://www.rulit.me/books/you-must-be-kidding-download-679486.html) \n",
    "9. [The Odyssey by Homer](https://www.gutenberg.org/ebooks/1727) \n",
    "10. [DOOM ON THE HILL](https://www.rulit.me/books/doom-on-the-hill-download-679371.html)\n",
    "11. [Simple Sabotage Field Manual by United States. Office of Strategic Services](https://www.gutenberg.org/ebooks/26184)\n",
    "12. [THE CHINESE DISKS](https://www.rulit.me/books/the-chinese-disks-download-679286.html) \n",
    "13. [The Mysterious Affair at Styles by Agatha Christie](https://www.gutenberg.org/ebooks/863) \n",
    "14. [Beowulf: An Anglo-Saxon Epic Poem by J. Lesslie Hall](https://www.gutenberg.org/ebooks/16328) \n",
    "15. [NEVER FALL FOR YOUR FIANCEE](https://www.rulit.me/books/never-fall-for-your-fiancee-download-678785.html)\n",
    "16. [Anna Karenina by graf Leo Tolstoy](https://www.gutenberg.org/ebooks/1399)\n",
    "17. [On Liberty by John Stuart Mill](https://www.gutenberg.org/ebooks/34901) \n",
    "18. [THE GARAUCAN SWINDLE](https://www.rulit.me/books/the-garaucan-swindle-download-678697.html) \n",
    "19. [SPOILS OF THE SHADOW](https://www.rulit.me/books/spoils-of-the-shadow-download-678653.html) \n",
    "20. [Old Granny Fox by Thornton W. Burgess](https://www.gutenberg.org/ebooks/4980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11506158"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'data/books.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ed McBain Give the Boys a Great Big Hand This is for Phyllis and Rick 1 It was raining. It had been raining for three days now, an ugly March rain that washed the brilliance of near-spring with a monochromatic, unrelenting gray. The television forecasters had correctly predicted rain for today and estimated that it would rain tomorrow also. Beyond that, they would not venture an opinion. But it seemed to Patrolman Richard Genero that it had been raining forever, and that it would continue to rain forever, and that eventually he would be washed away into the gutters and then carried into the sewers of Isola and dumped unceremoniously with the other garbage into either the River Harb or the River Dix. North or south, it didn’t make a damn bit of difference: both rivers were polluted; both stank of human waste. Like a man up to his ankles in water in a rapidly sinking rowboat, Genero stood on the corner and surveyed the near-empty streets. His rubber rain cape was as black and as shining\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(\" +\",\" \",text.replace(\"\\r\",\"\").replace(\"\\n\",\" \"))\n",
    "sample = text[:int(1e5)]\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №1\n",
    "Заменить все числа, которые представлены цифрами, их текстовым представлением (т.е. прописью). (1 = one, 23 = twenty three, 1042 = one thousand forty two, и т.п.). Методы библиотек не использовать, алгоритм необходимо написать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {\n",
    "    0:\"zero\",\n",
    "    1:\"one\",\n",
    "    2:\"two\",\n",
    "    3:\"three\",\n",
    "    4:\"four\",\n",
    "    5:\"five\",\n",
    "    6:\"six\",\n",
    "    7:\"seven\",\n",
    "    8:\"eight\",\n",
    "    9:\"nine\",\n",
    "    10:\"ten\",\n",
    "    11:\"eleven\",\n",
    "    12:\"twelve\",\n",
    "    13:\"thirteen\",\n",
    "    14:\"fourteen\",\n",
    "    15:\"fifteen\",\n",
    "    16:\"sixteen\",\n",
    "    17:\"seventeen\",\n",
    "    18:\"eigthteen\",\n",
    "    19:\"nineteen\",\n",
    "    20:\"twenty\",\n",
    "    30:\"thirty\",\n",
    "    40:\"fourty\",\n",
    "    50:\"fifty\",\n",
    "    60:\"sixty\",\n",
    "    70:\"seventy\",\n",
    "    80:\"eighty\",\n",
    "    90:\"ninety\"\n",
    "}\n",
    "def converter(num:str)->str:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num(str/int): input numeric\n",
    "    Return:\n",
    "        name in english for numeric\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        num_int = int(num)\n",
    "        if num_int <= 20:\n",
    "            return encoder[num_int]\n",
    "        elif num_int < 100:\n",
    "            decil, number = divmod(num_int,10)\n",
    "            if number:\n",
    "                return \"{} {}\".format(encoder[decil*10],encoder[number%10])\n",
    "            else:\n",
    "                return \"{}\".format(encoder[num_int])\n",
    "        else:\n",
    "            num = str(num)\n",
    "            num_lentgh = len(num)\n",
    "            if num_lentgh == 3:\n",
    "                if num_int // 100 and num_int % 100:\n",
    "                    return \"{} hundred {}\".format(converter(num[0]), converter(num[1:]))\n",
    "                else:\n",
    "                    return \"{} hundred\".format(converter(num[0]))\n",
    "            if num_lentgh == 4:\n",
    "                if num_int // 1000 and num_int % 100:\n",
    "                    return \"{} thousand {}\".format(converter(num[0]), converter(num[1:]))\n",
    "                else:\n",
    "                    return \"{} thousand\".format(converter(num[0]))   \n",
    "    except Exception as e:\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{331: 'three hundred thirty one',\n",
       " 1043: 'one thousand fourty three',\n",
       " 988: 'nine hundred eighty eight',\n",
       " 372: 'three hundred seventy two',\n",
       " 808: 'eight hundred eight',\n",
       " 347: 'three hundred fourty seven',\n",
       " 266: 'two hundred sixty six',\n",
       " 373: 'three hundred seventy three',\n",
       " 1459: 'one thousand four hundred fifty nine',\n",
       " 685: 'six hundred eighty five',\n",
       " 163: 'one hundred sixty three',\n",
       " 409: 'four hundred nine',\n",
       " 1044: 'one thousand fourty four',\n",
       " 1383: 'one thousand three hundred eighty three',\n",
       " 883: 'eight hundred eighty three'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_nums = random.sample(range(0, 1500), k=15)\n",
    "dict(zip(sample_nums, list(map(converter, sample_nums))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №2\n",
    "\n",
    "Преобразовать текст к нижнему или верхнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "sample = sample.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №3\n",
    "Произвести токенезацию (токенами выступают слова), не учитывая знаки препинания (т.е. необходимо разбить текст на токены (лексемы)):\n",
    "* С помощью nltk.tokenize.RegexpTokenizer (тут необходимо вспомнить регулярные выражения [posix](https://ru.wikibooks.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D1%8B%D0%B5_%D0%B2%D1%8B%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ed', 'mcbain', 'give', 'the', 'boys', 'a', 'great', 'big', 'hand', 'this', 'is', 'for', 'phyllis', 'and', 'rick', 'it', 'was', 'raining', 'it', 'had', 'been', 'raining', 'for', 'three', 'days', 'now', 'an', 'ugly', 'march', 'rain', 'that', 'washed', 'the', 'brilliance', 'of', 'near', 'spring', 'with', 'a', 'monochromatic', 'unrelenting', 'gray', 'the', 'television', 'forecasters', 'had', 'correctly', 'predicted', 'rain', 'for', 'today', 'and', 'estimated', 'that', 'it', 'would', 'rain', 'tomorrow', 'also', 'beyond', 'that', 'they', 'would', 'not', 'venture', 'an', 'opinion', 'but', 'it', 'seemed', 'to', 'patrolman', 'richard', 'genero', 'that', 'it', 'had', 'been', 'raining', 'forever', 'and', 'that', 'it', 'would', 'continue', 'to', 'rain', 'forever', 'and', 'that', 'eventually', 'he', 'would', 'be', 'washed', 'away', 'into', 'the', 'gutters', 'and']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_regexp = RegexpTokenizer(pattern=\"[^\\W\\d_]+\")\n",
    "tokens = tokenizer_regexp.tokenize(sample)\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "print(tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* С помощью nltk.word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ed', 'mcbain', 'give', 'the', 'boys', 'a', 'great', 'big', 'hand', 'this', 'is', 'for', 'phyllis', 'and', 'rick', 'it', 'was', 'raining', 'it', 'had', 'been', 'raining', 'for', 'three', 'days', 'now', 'an', 'ugly', 'march', 'rain', 'that', 'washed', 'the', 'brilliance', 'of', 'with', 'a', 'monochromatic', 'unrelenting', 'gray', 'the', 'television', 'forecasters', 'had', 'correctly', 'predicted', 'rain', 'for', 'today', 'and', 'estimated', 'that', 'it', 'would', 'rain', 'tomorrow', 'also', 'beyond', 'that', 'they', 'would', 'not', 'venture', 'an', 'opinion', 'but', 'it', 'seemed', 'to', 'patrolman', 'richard', 'genero', 'that', 'it', 'had', 'been', 'raining', 'forever', 'and', 'that', 'it', 'would', 'continue', 'to', 'rain', 'forever', 'and', 'that', 'eventually', 'he', 'would', 'be', 'washed', 'away', 'into', 'the', 'gutters', 'and', 'then', 'carried']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sample)\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №4\n",
    "Произвести частотный анализ слов. Методы библиотек не использовать, т.е. алгоритм необходимо написать самостоятельно. Cохранить результат в выходной csv файл в порядке убывания частот. Формат выходного файла:\n",
    "\n",
    "| Word | Count |\n",
    "| --- | --- |\n",
    "| cat | 1000 |\n",
    "| dog | 300 |\n",
    "| .... | .... |\n",
    "| butterfly | 34 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenFreq(tokens:str)->dict:\n",
    "    res = {}\n",
    "    for token in tokens:\n",
    "        if token in res.keys():\n",
    "            res[token] += 1\n",
    "        else:\n",
    "            res[token] = 1\n",
    "    return res\n",
    "\n",
    "def sortFreqDict(freqdict:dict)->list:\n",
    "    res = [(freqdict[key], key) for key in freqdict]\n",
    "    res.sort()\n",
    "    res.reverse()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1005, 'the'), (561, 'a'), (451, 'and'), (374, 'you'), (365, 'he'), (358, 'to'), (354, 'i'), (352, 'of'), (262, 'that'), (259, 'was'), (241, 'it'), (240, 'in'), (223, 'said'), (221, 's'), (143, 't'), (139, 'on'), (129, 'his'), (123, 'she'), (118, 'what'), (118, 'for')]\n"
     ]
    }
   ],
   "source": [
    "result = sortFreqDict(tokenFreq(tokens))\n",
    "print(result[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab1_step4.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"Word\",\"Count\"))\n",
    "    for tup in result:\n",
    "        writer.writerow(tup[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №5\n",
    "Произвести частотный анализ [N-грам](https://en.wikipedia.org/wiki/N-gram) и сохранить в выходной файл в порядке убывания частот (формат файла аналогичен предыдущему примеру, за исключением столбца Word, который заменяется на N-gram. В этой задаче можно использовать готовые библиотеки (смотрим состав библиотеки NLTK). Формат выходного файла:\n",
    "\n",
    "| N-gram | Count |\n",
    "| --- | --- |\n",
    "| cat is | 1000 |\n",
    "| is a | 300 |\n",
    "| .... | .... |\n",
    "| the fish | 34 |\n",
    "\n",
    "* Биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 98), (('in', 'the'), 83), (('on', 'the'), 58), (('to', 'the'), 57), (('carella', 'said'), 49), (('that', 's'), 47), (('do', 'you'), 42), (('don', 't'), 40), (('was', 'a'), 38), (('the', 'bag'), 38), (('it', 'was'), 37), (('i', 'm'), 36), (('he', 'was'), 36), (('and', 'then'), 34), (('of', 'a'), 34), (('and', 'he'), 32), (('in', 'a'), 31), (('at', 'the'), 31), (('it', 's'), 30), (('from', 'the'), 29), (('didn', 't'), 26), (('you', 'know'), 26), (('she', 'said'), 26), (('i', 'don'), 26), (('into', 'the'), 25), (('i', 'mean'), 24), (('and', 'the'), 24), (('with', 'a'), 23), (('there', 'was'), 23), (('he', 'had'), 23), (('out', 'of'), 22), (('he', 'said'), 22), (('the', 'hand'), 22), (('a', 'man'), 21), (('is', 'that'), 21), (('for', 'a'), 20), (('kling', 'said'), 20), (('you', 're'), 19), (('he', 's'), 19), (('said', 'you'), 19), (('had', 'been'), 18), (('a', 'little'), 18), (('you', 'mean'), 18), (('she', 'was'), 18), (('if', 'you'), 18), (('for', 'the'), 17), (('and', 'a'), 17), (('with', 'the'), 16), (('going', 'to'), 16), (('on', 'a'), 16), (('and', 'i'), 16), (('the', 'man'), 16), (('one', 'of'), 16), (('meyer', 'said'), 16), (('the', 'rain'), 15), (('have', 'to'), 15), (('ain', 't'), 15), (('would', 'you'), 15), (('to', 'be'), 15), (('got', 'a'), 15), (('you', 'think'), 14), (('t', 'know'), 13), (('isn', 't'), 13), (('let', 's'), 13), (('are', 'you'), 13), (('said', 'and'), 13), (('you', 'have'), 13), (('but', 'he'), 13), (('the', 'bus'), 13), (('did', 'not'), 13), (('did', 'you'), 13), (('the', 'room'), 13), (('the', 'other'), 12), (('to', 'get'), 12), (('that', 'the'), 12), (('the', 'door'), 12), (('said', 'i'), 12), (('i', 'll'), 12), (('it', 'he'), 12), (('have', 'been'), 12), (('and', 'she'), 12), (('kind', 'of'), 12), (('what', 'i'), 12), (('just', 'a'), 12), (('we', 're'), 12), (('you', 'can'), 12), (('that', 'it'), 11), (('and', 'that'), 11), (('like', 'a'), 11), (('his', 'head'), 11), (('he', 'd'), 11), (('about', 'it'), 11), (('a', 'cop'), 11), (('what', 'you'), 11), (('i', 'can'), 11), (('i', 'was'), 11), (('and', 'said'), 11), (('to', 'do'), 11), (('all', 'right'), 11), (('the', 'person'), 11)]\n"
     ]
    }
   ],
   "source": [
    "bigramm = Counter(ngrams(tokens, 2)).most_common()\n",
    "print(bigramm[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of the', 98)\n",
      "('in the', 83)\n",
      "('on the', 58)\n",
      "('to the', 57)\n",
      "('carella said', 49)\n",
      "('that s', 47)\n",
      "('do you', 42)\n",
      "('don t', 40)\n",
      "('was a', 38)\n",
      "('the bag', 38)\n",
      "('it was', 37)\n",
      "('i m', 36)\n",
      "('he was', 36)\n",
      "('and then', 34)\n",
      "('of a', 34)\n",
      "('and he', 32)\n",
      "('in a', 31)\n",
      "('at the', 31)\n",
      "('it s', 30)\n",
      "('from the', 29)\n",
      "('didn t', 26)\n",
      "('you know', 26)\n",
      "('she said', 26)\n",
      "('i don', 26)\n",
      "('into the', 25)\n",
      "('i mean', 24)\n",
      "('and the', 24)\n",
      "('with a', 23)\n",
      "('there was', 23)\n",
      "('he had', 23)\n",
      "('out of', 22)\n",
      "('he said', 22)\n",
      "('the hand', 22)\n",
      "('a man', 21)\n",
      "('is that', 21)\n",
      "('for a', 20)\n",
      "('kling said', 20)\n",
      "('you re', 19)\n",
      "('he s', 19)\n",
      "('said you', 19)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(\"lab1_step5_bigram.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"N-gram\",\"Count\"))\n",
    "    for tup in bigramm:\n",
    "        counter += 1\n",
    "        row = \" \".join(tup[0]), tup[1]\n",
    "        if counter <= 40:\n",
    "            print(row)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3-грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('i', 'don', 't'), 26), (('of', 'the', 'bag'), 12), (('do', 'you', 'mean'), 11), (('don', 't', 'know'), 11), (('out', 'of', 'the'), 9), (('he', 'did', 'not'), 9), (('do', 'you', 'think'), 8), (('a', 'meat', 'cleaver'), 8), (('i', 'wish', 'i'), 7), (('what', 'the', 'hell'), 7), (('if', 'you', 'are'), 7), (('i', 'm', 'going'), 6), (('you', 're', 'a'), 6), (('that', 's', 'what'), 6), (('do', 'you', 'know'), 6), (('the', 'middle', 'of'), 6), (('there', 'is', 'nothing'), 6), (('the', 'person', 'who'), 6), (('is', 'that', 'right'), 6), (('is', 'that', 'what'), 6), (('there', 'was', 'a'), 6), (('he', 'was', 'a'), 6), (('the', 'missing', 'persons'), 6), (('dozen', 'red', 'roses'), 6), (('m', 'going', 'to'), 5), (('t', 'have', 'to'), 5), (('what', 'do', 'you'), 5), (('a', 'long', 'time'), 5), (('it', 'ain', 't'), 5), (('said', 'and', 'he'), 5), (('shaking', 'his', 'head'), 5), (('what', 'kind', 'of'), 5), (('said', 'you', 're'), 5), (('he', 'didn', 't'), 5), (('what', 'i', 'mean'), 5), (('one', 'of', 'those'), 5), (('in', 'the', 'middle'), 5), (('you', 'are', 'a'), 5), (('are', 'a', 'cop'), 5), (('a', 'cop', 'you'), 5), (('the', 'severed', 'hand'), 5), (('said', 'it', 's'), 5), (('how', 'do', 'you'), 5), (('would', 'have', 'been'), 5), (('i', 'guess', 'i'), 5), (('are', 'you', 'aware'), 5), (('box', 'of', 'candy'), 5), (('do', 'you', 'have'), 5), (('that', 'what', 'you'), 5), (('a', 'dozen', 'red'), 5), (('it', 'was', 'still'), 4), (('back', 'to', 'the'), 4), (('ain', 't', 'got'), 4), (('day', 'like', 'this'), 4), (('he', 'had', 'to'), 4), (('i', 'mean', 'it'), 4), (('in', 'the', 'rain'), 4), (('on', 'the', 'sidewalk'), 4), (('the', 'bag', 'had'), 4), (('the', 'bag', 'and'), 4), (('could', 'have', 'been'), 4), (('i', 'had', 'a'), 4), (('the', 'bottom', 'of'), 4), (('bottom', 'of', 'the'), 4), (('i', 'didn', 't'), 4), (('you', 'know', 'what'), 4), (('know', 'what', 'i'), 4), (('i', 'mean', 'i'), 4), (('he', 'wasn', 't'), 4), (('get', 'it', 'out'), 4), (('it', 'out', 'of'), 4), (('of', 'the', 'room'), 4), (('that', 's', 'pretty'), 4), (('the', 'victim', 's'), 4), (('the', 'hand', 'was'), 4), (('and', 'a', 'half'), 4), (('belonged', 'to', 'a'), 4), (('didn', 't', 'have'), 4), (('yes', 'of', 'course'), 4), (('kind', 'of', 'a'), 4), (('i', 'would', 'say'), 4), (('don', 't', 'think'), 4), (('bag', 'on', 'the'), 4), (('i', 'got', 'a'), 4), (('you', 'didn', 't'), 4), (('well', 'that', 's'), 4), (('s', 'not', 'too'), 4), (('that', 'it', 'was'), 4), (('a', 'little', 'more'), 4), (('you', 'got', 'a'), 4), (('that', 's', 'all'), 4), (('well', 'you', 'know'), 4), (('into', 'the', 'room'), 4), (('she', 'said', 'and'), 4), (('isn', 't', 'it'), 4), (('he', 'was', 'very'), 4), (('them', 'in', 'a'), 4), (('yes', 'i', 'do'), 4), (('corner', 'of', 'the'), 4), (('of', 'an', 'inch'), 4)]\n"
     ]
    }
   ],
   "source": [
    "trigramm = Counter(ngrams(tokens, 3)).most_common()\n",
    "print(trigramm[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i don t', 26)\n",
      "('of the bag', 12)\n",
      "('do you mean', 11)\n",
      "('don t know', 11)\n",
      "('out of the', 9)\n",
      "('he did not', 9)\n",
      "('do you think', 8)\n",
      "('a meat cleaver', 8)\n",
      "('i wish i', 7)\n",
      "('what the hell', 7)\n",
      "('if you are', 7)\n",
      "('i m going', 6)\n",
      "('you re a', 6)\n",
      "('that s what', 6)\n",
      "('do you know', 6)\n",
      "('the middle of', 6)\n",
      "('there is nothing', 6)\n",
      "('the person who', 6)\n",
      "('is that right', 6)\n",
      "('is that what', 6)\n",
      "('there was a', 6)\n",
      "('he was a', 6)\n",
      "('the missing persons', 6)\n",
      "('dozen red roses', 6)\n",
      "('m going to', 5)\n",
      "('t have to', 5)\n",
      "('what do you', 5)\n",
      "('a long time', 5)\n",
      "('it ain t', 5)\n",
      "('said and he', 5)\n",
      "('shaking his head', 5)\n",
      "('what kind of', 5)\n",
      "('said you re', 5)\n",
      "('he didn t', 5)\n",
      "('what i mean', 5)\n",
      "('one of those', 5)\n",
      "('in the middle', 5)\n",
      "('you are a', 5)\n",
      "('are a cop', 5)\n",
      "('a cop you', 5)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(\"lab1_step5_threegram.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"N-gram\",\"Count\"))\n",
    "    for tup in trigramm:\n",
    "        counter += 1\n",
    "        row = \" \".join(tup[0]), tup[1]\n",
    "        if counter <= 40:\n",
    "            print(row)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №6 (аналогично шагу 3)\n",
    "Произвести токенезацию, но уже учитывая знаки препинания:\n",
    "* С помощью nltk.tokenize.RegexpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ed', 'mcbain', 'give', 'the', 'boys', 'a', 'great', 'big', 'hand', 'this', 'is', 'for', 'phyllis', 'and', 'rick', 'it', 'was', 'raining', '.', 'it', 'had', 'been', 'raining', 'for', 'three', 'days', 'now', ',', 'an', 'ugly', 'march', 'rain', 'that', 'washed', 'the', 'brilliance', 'of', 'near', 'spring', 'with', 'a', 'monochromatic', ',', 'unrelenting', 'gray', '.', 'the', 'television', 'forecasters', 'had', 'correctly', 'predicted', 'rain', 'for', 'today', 'and', 'estimated', 'that', 'it', 'would', 'rain', 'tomorrow', 'also', '.', 'beyond', 'that', ',', 'they', 'would', 'not', 'venture', 'an', 'opinion', '.', 'but', 'it', 'seemed', 'to', 'patrolman', 'richard', 'genero', 'that', 'it', 'had', 'been', 'raining', 'forever', ',', 'and', 'that', 'it', 'would', 'continue', 'to', 'rain', 'forever', ',', 'and', 'that', 'eventually']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_regexp = RegexpTokenizer(pattern=\"[^\\W\\d_]+|\\.|,|!|\\?\")\n",
    "tokens = tokenizer_regexp.tokenize(sample)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* С помощью nltk.word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ed', 'mcbain', 'give', 'the', 'boys', 'a', 'great', 'big', 'hand', 'this', 'is', 'for', 'phyllis', 'and', 'rick', '1', 'it', 'was', 'raining', '.', 'it', 'had', 'been', 'raining', 'for', 'three', 'days', 'now', ',', 'an', 'ugly', 'march', 'rain', 'that', 'washed', 'the', 'brilliance', 'of', 'near', '-', 'spring', 'with', 'a', 'monochromatic', ',', 'unrelenting', 'gray', '.', 'the', 'television', 'forecasters', 'had', 'correctly', 'predicted', 'rain', 'for', 'today', 'and', 'estimated', 'that', 'it', 'would', 'rain', 'tomorrow', 'also', '.', 'beyond', 'that', ',', 'they', 'would', 'not', 'venture', 'an', 'opinion', '.', 'but', 'it', 'seemed', 'to', 'patrolman', 'richard', 'genero', 'that', 'it', 'had', 'been', 'raining', 'forever', ',', 'and', 'that', 'it', 'would', 'continue', 'to', 'rain', 'forever', ',', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(sample)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №7 (аналогично шагу 4)\n",
    "Произвести частотный анализ слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenFreq(tokens:str)->dict:\n",
    "    res = {}\n",
    "    for token in tokens:\n",
    "        if token in res.keys():\n",
    "            res[token] += 1\n",
    "        else:\n",
    "            res[token] = 1\n",
    "    return res\n",
    "\n",
    "def sortFreqDict(freqdict:dict)->list:\n",
    "    res = [(freqdict[key], key) for key in freqdict]\n",
    "    res.sort()\n",
    "    res.reverse()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1640, '.'), (1462, ','), (1005, 'the'), (568, 'a'), (453, 'and'), (445, '?'), (382, 'you'), (366, 'he'), (359, 'to'), (355, 'i'), (353, 'of'), (266, 'that'), (261, 'was'), (245, 'it'), (242, 'in'), (224, 'said'), (223, 's'), (144, 't'), (139, 'on'), (129, 'his')]\n"
     ]
    }
   ],
   "source": [
    "result = sortFreqDict(tokenFreq(tokens))\n",
    "print(result[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab1_step7.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"Word\",\"Count\"))\n",
    "    for tup in result:\n",
    "        writer.writerow(tup[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Шаг №8 (аналогично шагу 5)\n",
    "Произвести частотный анализ N-грам.\n",
    "\n",
    "* Биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', 'and'), 167), (('said', '.'), 135), (('.', 'i'), 119), (('.', 'the'), 100), (('of', 'the'), 98), (('.', 'he'), 97), (('in', 'the'), 83), ((',', 'he'), 72), (('.', 'you'), 66), (('.', 'and'), 60), (('on', 'the'), 58), ((',', 'i'), 58), (('to', 'the'), 57), (('.', '.'), 55), (('well', ','), 52), (('said', ','), 50), (('?', 'i'), 50), ((',', 'carella'), 50), (('carella', 'said'), 50), ((',', 'the'), 49), (('mrs', '.'), 48), (('that', 's'), 47), (('.', 'it'), 46), (('.', 'she'), 46), ((',', 'you'), 45), (('?', 'yes'), 44), (('yes', ','), 43), (('do', 'you'), 42), (('don', 't'), 40), (('.', 'well'), 39), (('the', 'bag'), 39), (('no', ','), 39), (('was', 'a'), 38), (('.', 'what'), 38), (('it', 'was'), 37), ((',', 'but'), 37), (('he', 'was'), 37), ((',', 'mrs'), 37), (('and', 'then'), 36), (('i', 'm'), 36), (('of', 'a'), 35), ((',', 'that'), 34), (('.', 'but'), 32), (('and', 'he'), 32), (('.', 'we'), 32), ((',', 'sir'), 32), (('?', 'no'), 32), (('in', 'a'), 31), (('.', 'a'), 31), (('you', 'know'), 31), (('.', 'that'), 31), (('at', 'the'), 31), (('it', 's'), 30), (('yes', '.'), 30), (('it', '.'), 29), (('from', 'the'), 29), ((',', 'she'), 28), ((',', 'kling'), 28), (('.', 'androvich'), 28), (('it', '?'), 27), (('asked', '.'), 27), (('didn', 't'), 26), (('she', 'said'), 26), (('i', 'don'), 26), (('into', 'the'), 25), ((',', 'genero'), 25), (('i', 'mean'), 25), ((',', 'a'), 25), (('and', 'the'), 24), (('.', 'yes'), 24), (('sir', '.'), 24), (('there', 'was'), 23), (('he', 'had'), 23), (('?', 'he'), 23), (('with', 'a'), 22), ((',', 'it'), 22), (('a', 'man'), 22), (('out', 'of'), 22), (('he', 'said'), 22), (('.', 'in'), 22), (('the', 'hand'), 22), (('.', 'no'), 21), (('is', 'that'), 21), (('for', 'a'), 20), (('.', 'so'), 20), ((',', 'meyer'), 20), (('kling', 'said'), 20), (('the', 'rain'), 19), (('you', 're'), 19), (('.', 'yeah'), 19), (('he', 's'), 19), ((',', 'lennie'), 19), (('had', 'been'), 18), (('a', 'little'), 18), (('mean', ','), 18), (('it', ','), 18), (('and', 'a'), 18), (('you', 'mean'), 18), (('.', 'do'), 18), (('.', 'there'), 18)]\n"
     ]
    }
   ],
   "source": [
    "bigramm = Counter(ngrams(tokens, 2)).most_common()\n",
    "print(bigramm[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(', and', 167)\n",
      "('said .', 135)\n",
      "('. i', 119)\n",
      "('. the', 100)\n",
      "('of the', 98)\n",
      "('. he', 97)\n",
      "('in the', 83)\n",
      "(', he', 72)\n",
      "('. you', 66)\n",
      "('. and', 60)\n",
      "('on the', 58)\n",
      "(', i', 58)\n",
      "('to the', 57)\n",
      "('. .', 55)\n",
      "('well ,', 52)\n",
      "('said ,', 50)\n",
      "('? i', 50)\n",
      "(', carella', 50)\n",
      "('carella said', 50)\n",
      "(', the', 49)\n",
      "('mrs .', 48)\n",
      "('that s', 47)\n",
      "('. it', 46)\n",
      "('. she', 46)\n",
      "(', you', 45)\n",
      "('? yes', 44)\n",
      "('yes ,', 43)\n",
      "('do you', 42)\n",
      "('don t', 40)\n",
      "('. well', 39)\n",
      "('the bag', 39)\n",
      "('no ,', 39)\n",
      "('was a', 38)\n",
      "('. what', 38)\n",
      "('it was', 37)\n",
      "(', but', 37)\n",
      "('he was', 37)\n",
      "(', mrs', 37)\n",
      "('and then', 36)\n",
      "('i m', 36)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(\"lab1_step8_bigram.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"N-gram\",\"Count\"))\n",
    "    for tup in bigramm:\n",
    "        counter += 1\n",
    "        row = \" \".join(tup[0]), tup[1]\n",
    "        if counter <= 40:\n",
    "            print(row)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3-грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', 'carella', 'said'), 43), (('carella', 'said', '.'), 41), (('.', 'well', ','), 37), ((',', 'mrs', '.'), 37), ((',', 'and', 'he'), 28), (('.', '.', '.'), 28), (('mrs', '.', 'androvich'), 28), (('i', 'don', 't'), 26), ((',', 'sir', '.'), 24), (('?', 'yes', '.'), 23), (('?', 'no', ','), 21), ((',', 'that', 's'), 20), (('?', 'yes', ','), 20), (('.', 'that', 's'), 19), ((',', 'kling', 'said'), 19), ((',', 'she', 'said'), 18), (('mrs', '.', 'livingston'), 18), (('.', 'i', 'm'), 17), ((',', 'and', 'then'), 17), ((',', 'he', 'said'), 17), (('.', 'do', 'you'), 17), (('i', 'mean', ','), 16), (('.', 'yes', ','), 16), (('.', 'it', 'was'), 14), (('.', 'it', 's'), 14), (('he', 'said', '.'), 14), ((',', 'you', 'know'), 14), (('?', 'i', 'don'), 14), (('don', 't', 'know'), 14), (('.', 'he', 'was'), 13), (('said', '.', 'you'), 13), ((',', 'meyer', 'said'), 13), (('kling', 'said', '.'), 13), (('she', 'said', '.'), 13), (('.', 'i', 'mean'), 12), (('of', 'the', 'bag'), 12), (('.', 'oh', ','), 12), (('.', 'yeah', ','), 12), (('?', 'well', ','), 12), (('.', 'androvich', '?'), 12), (('and', 'said', ','), 11), (('yes', ',', 'sir'), 11), (('.', 'there', 'was'), 11), ((',', 'and', 'she'), 11), (('.', 'no', ','), 11), (('do', 'you', 'mean'), 11), ((',', 'he', 'thought'), 10), (('you', 'know', '.'), 10), ((',', 'i', 'm'), 10), (('the', 'bag', '.'), 10), ((',', 'is', 'that'), 10), (('meyer', 'said', '.'), 10), (('?', 'no', '.'), 10), (('.', 'androvich', '.'), 10), (('out', 'of', 'the'), 9), (('said', ',', 'and'), 9), ((',', 'but', 'he'), 9), (('he', 'did', 'not'), 9), (('.', 'he', 'had'), 9), (('.', 'if', 'you'), 9), ((',', 'and', 'the'), 9), ((',', 'lennie', '?'), 9), (('do', 'you', 'think'), 8), ((',', 'genero', 'said'), 8), (('said', '.', 'the'), 8), ((',', 'and', 'i'), 8), (('.', 'she', 'was'), 8), (('that', 's', 'all'), 8), (('is', 'it', '?'), 8), ((',', 'of', 'course'), 8), (('a', 'meat', 'cleaver'), 8), ((',', 'and', 'that'), 7), (('he', 'thought', '.'), 7), (('a', 'cop', ','), 7), (('i', 'wish', 'i'), 7), (('.', 'would', 'you'), 7), (('.', 'you', 're'), 7), ((',', 'sir', ','), 7), ((',', 'it', 'was'), 7), (('said', '.', 'i'), 7), ((',', 'do', 'you'), 7), (('carella', 'asked', '.'), 7), ((',', 'too', '.'), 7), (('?', 'carella', 'asked'), 7), (('.', 'i', 'll'), 7), (('what', 'the', 'hell'), 7), (('the', 'room', '.'), 7), (('if', 'you', 'are'), 7), ((',', 'yes', ','), 7), (('.', 'you', 'know'), 7), ((',', 'genero', '?'), 7), (('.', 'i', 'see'), 7), (('?', 'carella', 'said'), 7), (('.', 'she', 'paused'), 7), (('.', 'i', 'don'), 7), (('.', 'mrs', '.'), 7), (('dozen', 'red', 'roses'), 7), (('.', 'livingston', '?'), 7), ((',', 'too', ','), 6), (('i', 'm', 'going'), 6)]\n"
     ]
    }
   ],
   "source": [
    "trigramm = Counter(ngrams(tokens, 3)).most_common()\n",
    "print(trigramm[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(', carella said', 43)\n",
      "('carella said .', 41)\n",
      "('. well ,', 37)\n",
      "(', mrs .', 37)\n",
      "(', and he', 28)\n",
      "('. . .', 28)\n",
      "('mrs . androvich', 28)\n",
      "('i don t', 26)\n",
      "(', sir .', 24)\n",
      "('? yes .', 23)\n",
      "('? no ,', 21)\n",
      "(', that s', 20)\n",
      "('? yes ,', 20)\n",
      "('. that s', 19)\n",
      "(', kling said', 19)\n",
      "(', she said', 18)\n",
      "('mrs . livingston', 18)\n",
      "('. i m', 17)\n",
      "(', and then', 17)\n",
      "(', he said', 17)\n",
      "('. do you', 17)\n",
      "('i mean ,', 16)\n",
      "('. yes ,', 16)\n",
      "('. it was', 14)\n",
      "('. it s', 14)\n",
      "('he said .', 14)\n",
      "(', you know', 14)\n",
      "('? i don', 14)\n",
      "('don t know', 14)\n",
      "('. he was', 13)\n",
      "('said . you', 13)\n",
      "(', meyer said', 13)\n",
      "('kling said .', 13)\n",
      "('she said .', 13)\n",
      "('. i mean', 12)\n",
      "('of the bag', 12)\n",
      "('. oh ,', 12)\n",
      "('. yeah ,', 12)\n",
      "('? well ,', 12)\n",
      "('. androvich ?', 12)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(\"lab1_step8_threegram.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"N-gram\",\"Count\"))\n",
    "    for tup in trigramm:\n",
    "        counter += 1\n",
    "        row = \" \".join(tup[0]), tup[1]\n",
    "        if counter <= 40:\n",
    "            print(row)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
